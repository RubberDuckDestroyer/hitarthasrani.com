{"componentChunkName":"component---src-templates-blog-post-js","path":"/2023-03-06-bulk-s3-bucket-transfer/","result":{"data":{"site":{"siteMetadata":{"title":"Hitarth Asrani","author":"Hitarth Asrani"}},"markdownRemark":{"id":"f0a31b75-323e-5bfe-b660-834547c3a839","excerpt":"In my opinion, AWS S3 is by far the most popular and widely used services. I dare say it is the most versatile AWS service. There’s so much you can do - general…","html":"<p>In my opinion, AWS S3 is by far the most popular and widely used services. I dare say it is the most versatile AWS service. There’s so much you can do - general object storage, use it for static file hosting/ content delivery, use it as a service for backups, store and query your log data with SQL using Athena or archive all your data using S3 Glacier and S3 Glacier deep archive. Naturally, a lot of companies want to make the most out of cloud storage  and decrease the cost spent on storing IT information on premisis. There’s a lot of data in the world, just look at Facebook…errr…Meta (or any other tech company?) and the concerns governments have about data privacy.</p>\n<h2>The Situation?</h2>\n<ul>\n<li>Let’s say you are utilising S3 buckets to store files…lots of files…not “I am big tech” kind of files, rather medium to large enterprise kind of files. What kind of files you ask? Maybe text files, json files or anything else. Each file is somewhere between a few hundred KBs to a couple of MBs  But give me a number of files…ok we got around 75,000 files.</li>\n<li>Now you also want to move <em>some</em> (74,500) files from one folder (ie. ‘folder1’) to a new folder (ie. ‘folder1_old’) based on when they were uploaded. SO THAT YOU CAN CLEAN UP THIS FOLDER AND HAVE ONLY FILES UPLOADED FROM TODAY. This is easy with the S3 console when you have a few files (say less than a 1000) but the option goes away once you cross the 999+ files mark. Check the images below for an example.\n<img src=\"\" alt=\"&#x27;sorting less than 999 files&#x27;\"></li>\n</ul>\n<p><img src=\"\" alt=\"&#x27;sorting more than 999 files&#x27;\"></p>\n<ul>\n<li>For convenience sake, you have some prefixes you can use to filter some files, <em>sometimes</em> but there’s also <em>20 or more different prefixes</em> because why not. Why? because you, my dear reader, <strong>also</strong> have to transfer some files to another folder every couple of hours based on the prefixes. These files get processed and moved using s3 events. <em>Note that this automation using s3 events and lambda is also in place but inactive until tomorrow or when you start it</em>. You decide to do this manually because you didnt know there were 75,000 files.</li>\n<li>We’re going production coz of course we are. AKA Time pressure and clogged thinking.</li>\n<li>The cherry on top: every hour you get 80 - 100 files more.</li>\n</ul>\n<p><strong>TLDR, tell me the situation shortly</strong>\n<img src=\"\" alt=\"what a situation\"></p>\n<h2>Not to do? Still a viable option but there might be a better way</h2>\n<p>Step 1: Immidiately transfer any files that need to be processed into the “process_me” folder . (If you take more than an hour you will have to do this again)</p>\n<ul>\n<li>Do this manually if needed. VERY HIGHLY DO NOT RECOMMEND THIS APPROACH\nOR</li>\n<li>Do an <code class=\"language-text\">aws s3 ls --summarize</code> and save the output as a text file. (The time taken for this depends on the number of objects in that bucket/folder)</li>\n<li>Import the text file into Excel, using only the columns for “Upload Date” and “object name/file name”</li>\n<li>Sort the excel sheet using the date to find out what files were uploaded today.</li>\n<li>Copy the names of all files uploaded today into a text file.</li>\n<li>Use ChatGPT to write a script that copies files from your bucket’s folder1 to test, using above text file as input for filenames. Make some changes/fixes if needed. End result of script is:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">script here</code></pre></div>\n<p>Note: this script can be modified later to do other things</p>\n<p>Repeat this step if the clock has past the hour, more files have been sent to this folder.</p>\n<p>Step 2: If this is done before the clock strikes the next hour ^*:</p>\n<ul>\n<li>do a <code class=\"language-text\">s3 sync &lt;source> &lt;destination></code> to copy all missing files/sync all files from “folder” into the “folder1_old” folder.</li>\n<li>do a <code class=\"language-text\">s3 rm bucket/folder --include \"*\" --exclude \"file1.do_not_forget_extension\" --exclude \"file2.do_not_forget_extension\" ..... --exclude \"last_file_to_exclude.do_not_forget_extension\"</code> To remove all files that were just copied to folder1_old if all of them were copied successfully.\nWhen using this script to remove the files, there’s some thing to remember: you need to add the file extension in the exclude option and you need to manually or magically(using another script) add all the files to exclude as exclude options into a command that you run.</li>\n</ul>\n<p>Step 3: Enable automation to process files from folder1 now.</p>\n<h2>Alternatives?</h2>\n<p>I did not look into using S3 batch operations, but there might be some kind of solution there. But you might also need replication set up prior to that?</p>\n<p>Some other weird script could also be a potential solution?</p>","frontmatter":{"title":"Moving 75,000 files in S3","date":"March 06, 2023","description":"To do and not to do when moving a lot of files between S3.","videoSrcURL":null,"videoTitle":null}}},"pageContext":{"slug":"/2023-03-06-bulk-s3-bucket-transfer/","previous":{"fields":{"slug":"/2022-11-25-aws-windows-procstat-monitoring/"},"frontmatter":{"title":"Process Monitoring on Windows EC2 Instances With Procstat"}},"next":{"fields":{"slug":"/2023-03-13-aws-direct-connect-auckland/"},"frontmatter":{"title":"Using the New AWS Auckland Location in AWS Direct Connect"}}}},"staticQueryHashes":["4123550546","63159454"]}